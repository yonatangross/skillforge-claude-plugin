{
  "$schema": "../../../../../.claude/schemas/skill-capabilities.schema.json",
  "name": "ollama-local",
  "version": "1.0.0",
  "description": "Local LLM inference with Ollama for development and CI",
  "capabilities": [
    "model-selection",
    "provider-template",
    "setup"
  ],
  "triggers": {
    "high_confidence": [
      "ollama",
      "local.*llm",
      "self.*hosted.*model"
    ],
    "medium_confidence": [
      "local.*inference",
      "llama.*local"
    ]
  },
  "integrates_with": [
    "llm-streaming",
    "embeddings",
    "llm-evaluation"
  ]
}
