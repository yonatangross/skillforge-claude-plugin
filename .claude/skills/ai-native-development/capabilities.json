{
  "$schema": "../../schemas/skill-capabilities.schema.json",
  "name": "ai-native-development",
  "version": "1.2.0",
  "description": "RAG pipelines, embeddings, vector databases, agentic workflows, LLM integration, local inference with Ollama, and cost optimization",

  "capabilities": {
    "embeddings": {
      "keywords": ["embedding", "vector", "semantic search", "similarity", "openai embedding"],
      "solves": [
        "How do I create embeddings?",
        "Semantic search implementation",
        "Compare text similarity"
      ],
      "reference_file": "references/vector-databases.md",
      "token_cost": 180
    },
    "rag": {
      "keywords": ["rag", "retrieval", "knowledge base", "document qa", "context retrieval"],
      "solves": [
        "How do I build a RAG pipeline?",
        "Q&A over documents",
        "Knowledge base chatbot"
      ],
      "reference_file": "references/rag-patterns.md",
      "token_cost": 250
    },
    "vector-database": {
      "keywords": ["pinecone", "chroma", "weaviate", "qdrant", "vector db", "pgvector"],
      "solves": [
        "Which vector database should I use?",
        "Set up Pinecone/Chroma",
        "Store and query embeddings"
      ],
      "reference_file": "references/vector-databases.md",
      "token_cost": 200
    },
    "function-calling": {
      "keywords": ["function calling", "tool use", "tool call", "json schema", "structured output"],
      "solves": [
        "How do I use function calling?",
        "Define tools for LLM",
        "Structured LLM output"
      ],
      "reference_file": "references/function-calling.md",
      "token_cost": 200
    },
    "agentic-workflows": {
      "keywords": ["agent", "react", "langchain", "langgraph", "multi-agent", "autonomous"],
      "solves": [
        "How do I build an AI agent?",
        "ReAct pattern implementation",
        "Multi-agent coordination"
      ],
      "reference_file": "references/agentic-workflows.md",
      "token_cost": 300
    },
    "cost-optimization": {
      "keywords": ["token", "cost", "budget", "cache", "model selection", "pricing"],
      "solves": [
        "How do I reduce LLM costs?",
        "Token counting and budgeting",
        "Prompt caching strategies"
      ],
      "reference_file": "references/observability.md#cost-optimization",
      "token_cost": 150
    },
    "observability": {
      "keywords": ["langfuse", "tracing", "monitoring", "evaluation", "llm observability"],
      "solves": [
        "How do I monitor LLM performance?",
        "Set up Langfuse tracing",
        "Evaluate AI quality"
      ],
      "reference_file": "references/observability.md",
      "token_cost": 180
    },
    "local-inference": {
      "keywords": ["ollama", "local llm", "self-hosted", "deepseek", "qwen", "llama", "local model", "ci cost"],
      "solves": [
        "How do I run LLMs locally?",
        "Reduce CI/CD costs with local models",
        "Set up Ollama for development",
        "Use DeepSeek or Qwen locally"
      ],
      "reference_file": "references/ollama-local-inference.md",
      "token_cost": 250
    }
  },

  "triggers": {
    "high_confidence": ["rag.*pipeline", "llm.*integration", "ai.*agent", "embedding.*search", "ollama", "local.*model", "deepseek", "qwen"],
    "medium_confidence": ["chatbot", "ai.*feature", "openai.*api", "ci.*cost", "self.*hosted"]
  },

  "integrates_with": ["streaming-api-patterns", "observability-monitoring", "type-safety-validation", "llm-caching-patterns"],

  "progressive_loading": {
    "tier_1_discovery": {
      "file": "capabilities.json",
      "tokens": 100,
      "use_when": "Initial task analysis - determine if skill is relevant"
    },
    "tier_2_overview": {
      "file": "SKILL.md",
      "tokens": 700,
      "use_when": "Skill confirmed relevant - need AI/ML patterns and best practices"
    },
    "tier_3_specific": {
      "files": "references/*.md",
      "tokens": "150-300 each",
      "use_when": "Need specific implementation guidance (RAG, embeddings, agents)"
    },
    "tier_4_generate": {
      "files": ["templates/*.ts", "templates/*.py"],
      "tokens": "200-400 each",
      "use_when": "Ready to generate RAG pipeline, agentic workflow, or Ollama provider code"
    }
  },

  "mcp_tools": {
    "documentation_lookup": {
      "tool": "context7",
      "library_ids": ["/langchain-ai/langgraph", "/anthropics/anthropic-sdk-python", "/openai/openai-python", "/langchain-ai/langchain"],
      "use_when": "Need current LLM framework patterns and APIs"
    },
    "observability": {
      "tool": "skillforge-langfuse",
      "use_when": "Setting up LLM tracing and evaluation"
    }
  }
}
