{
  "$schema": "../../schemas/skill-capabilities.schema.json",
  "name": "llm-safety-patterns",
  "version": "1.0.0",
  "description": "Patterns for secure LLM integration - context separation, prompt safety, output validation, and hallucination prevention",

  "capabilities": {
    "context-separation": {
      "keywords": ["context separation", "prompt context", "id in prompt", "parameterized"],
      "solves": [
        "How do I prevent IDs from leaking into prompts?",
        "How do I separate system context from prompt content?",
        "What should never appear in LLM prompts?"
      ],
      "reference_file": "references/context-separation.md",
      "token_cost": 350
    },
    "pre-llm-filtering": {
      "keywords": ["pre-llm", "rag filter", "data filter", "tenant filter"],
      "solves": [
        "How do I filter data before sending to LLM?",
        "How do I ensure tenant isolation in RAG?",
        "How do I scope retrieval to current user?"
      ],
      "reference_file": "references/pre-llm-filtering.md",
      "token_cost": 250
    },
    "post-llm-attribution": {
      "keywords": ["attribution", "source tracking", "provenance", "citation"],
      "solves": [
        "How do I track which sources the LLM used?",
        "How do I attribute results correctly?",
        "How do I avoid LLM-generated IDs?"
      ],
      "reference_file": "references/post-llm-attribution.md",
      "token_cost": 200
    },
    "output-guardrails": {
      "keywords": ["guardrail", "output validation", "hallucination", "toxicity"],
      "solves": [
        "How do I validate LLM output?",
        "How do I detect hallucinations?",
        "How do I prevent toxic content generation?"
      ],
      "reference_file": "references/output-guardrails.md",
      "token_cost": 300
    },
    "prompt-audit": {
      "keywords": ["prompt audit", "prompt security", "prompt injection"],
      "solves": [
        "How do I verify no IDs leaked to prompts?",
        "How do I audit prompts for security?",
        "How do I prevent prompt injection?"
      ],
      "reference_file": "references/prompt-audit.md",
      "token_cost": 200
    }
  },

  "skillforge_forbidden_in_prompts": [
    "user_id",
    "tenant_id",
    "analysis_id",
    "document_id",
    "artifact_id",
    "chunk_id",
    "session_id",
    "trace_id",
    "workflow_run_id",
    "api_key",
    "any UUID pattern"
  ],

  "triggers": {
    "high_confidence": ["llm.*safety", "prompt.*injection", "hallucination", "context.*separation", "id.*prompt"],
    "medium_confidence": ["llm.*security", "prompt.*audit", "output.*validation"]
  },

  "integrates_with": ["defense-in-depth", "security-checklist", "ai-native-development"],

  "progressive_loading": {
    "tier_1_discovery": {
      "file": "capabilities.json",
      "tokens": 100,
      "use_when": "Initial task analysis"
    },
    "tier_2_overview": {
      "file": "SKILL.md",
      "tokens": 500,
      "use_when": "Need LLM safety overview"
    },
    "tier_3_specific": {
      "files": "references/*.md",
      "tokens": "200-350 each",
      "use_when": "Need specific pattern implementation"
    },
    "tier_4_generate": {
      "files": "templates/*.py",
      "tokens": "150-250 each",
      "use_when": "Ready to generate safe LLM code"
    }
  }
}
