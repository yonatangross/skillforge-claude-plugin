# LLM Safety Checklist

## Input Safety

- [ ] Validate input length
- [ ] Detect prompt injection attempts
- [ ] Sanitize user content
- [ ] Rate limit requests

## Output Safety

- [ ] Content filtering
- [ ] PII detection and redaction
- [ ] Harmful content detection
- [ ] Bias monitoring

## System Prompts

- [ ] Clear boundaries
- [ ] Role definition
- [ ] Refusal instructions
- [ ] No secrets in prompts

## Guardrails

- [ ] Input guardrails
- [ ] Output guardrails
- [ ] Topic restrictions
- [ ] Sensitive content handling

## Monitoring

- [ ] Log flagged content
- [ ] Alert on violations
- [ ] Human review queue
- [ ] Incident response plan
